{"id":"d5061d6f-c9b8-4181-9304-51ac1cf405e9","data":{"nodes":[{"id":"TextOutput-y81Tk","type":"genericNode","position":{"x":4384.583409602494,"y":2153.0337197878052},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.1.0"},"id":"TextOutput-y81Tk"},"selected":false,"width":384,"height":289,"dragging":false},{"id":"TextInput-yoLpb","type":"genericNode","position":{"x":1217.0349407338745,"y":2371.127877492604},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"{\"cfg\": {\"algorithm\": \"hota\", \"max_fe\": 100, \"pop_size\": 10, \"init_pop_size\": 30, \"mutation_rate\": 0.5, \"timeout\": 10, \"diversify_init_pop\": true, \"problem\": {\"problem_name\": \"mkp_aco\", \"problem_type\": \"aco\", \"obj_type\": \"max\", \"problem_size\": 100, \"func_name\": \"heuristics\", \"description\": \"Solving Multiple Knapsack Problems (MKP) through stochastic solution sampling based on \\\"heuristics\\\". MKP involves selecting a subset of items to maximize the total prize collected, subject to multi-dimensional maximum weight constraints.\"}, \"llm_client\": {\"_target_\": \"utils.llm_client.openai.OpenAIClient\", \"model\": \"gpt-3.5-turbo\", \"temperature\": 1.0, \"api_key\": null}}, \"info\": {\"problem\": \"mkp_aco\", \"problem_desc\": \"Solving Multiple Knapsack Problems (MKP) through stochastic solution sampling based on \\\"heuristics\\\". MKP involves selecting a subset of items to maximize the total prize collected, subject to multi-dimensional maximum weight constraints.\", \"func_name\": \"heuristics\", \"func_desc\": \"Suppose `n` indicates the scale of the problem, and `m` is the dimension of weights each item has. The constraint of each dimension is fixed to 1. The `heuristics` function takes as input a `prize` of shape (n,), a `weight` of shape (n, m), and returns `heuristics` of shape (n,). `heuristics[i]` indicates how promising it is to include item i in the solution.\", \"seed_func\": \"def heuristics_v1(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n    return prize / np.sum(weight, axis=1)\\n\", \"func_signature\": \"def heuristics_v{version}(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\", \"external_knowledge\": \"- Try combining various factors to determine how promising it is to select an item.\\n- Try sparsifying the heuristics by setting unpromising elements to zero.\", \"system_generator_prompt\": \"You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\\nYour response outputs Python code and nothing else. Format your code as a Python code string: \\\"```python ... ```\\\".\\n\", \"system_reflector_prompt\": \"You are an expert in the domain of optimization heuristics. Your task is to give hints to design better heuristics.\\n\", \"user_reflector_st_prompt\": \"Below are two {func_name} functions for {problem_desc}\\n{func_desc}\\n\\nYou are provided with two code versions below, where the second version performs better than the first one.\\n\\n[Worse code]\\n{worse_code}\\n\\n[Better code]\\n{better_code}\\n\\nYou respond with some hints for designing better heuristics, based on the two code versions and using less than 20 words.\", \"user_reflector_lt_prompt\": \"Below is your prior long-term reflection on designing heuristics for {problem_desc}\\n{prior_reflection}\\n\\nBelow are some newly gained insights.\\n{new_reflection}\\n\\nWrite constructive hints for designing better heuristics, based on prior reflections and new insights and using less than 50 words.\", \"crossover_prompt\": \"{user_generator}\\n\\n[Worse code]\\n{func_signature0}\\n{worse_code}\\n\\n[Better code]\\n{func_signature1}\\n{better_code}\\n\\n[Reflection]\\n{reflection}\\n\\n[Improved code]\\nPlease write an improved function `{func_name}_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```.\", \"mutataion_prompt\": \"{user_generator}\\n\\n[Prior reflection]\\n{reflection}\\n\\n[Code]\\n{func_signature1}\\n{elitist_code}\\n\\n[Improved code]\\nPlease write a mutated function `{func_name}_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```.\", \"user_generator_prompt\": \"Write a heuristics function for Solving Multiple Knapsack Problems (MKP) through stochastic solution sampling based on \\\"heuristics\\\". MKP involves selecting a subset of items to maximize the total prize collected, subject to multi-dimensional maximum weight constraints.\\nSuppose `n` indicates the scale of the problem, and `m` is the dimension of weights each item has. The constraint of each dimension is fixed to 1. The `heuristics` function takes as input a `prize` of shape (n,), a `weight` of shape (n, m), and returns `heuristics` of shape (n,). `heuristics[i]` indicates how promising it is to include item i in the solution.\\n\", \"seed_prompt\": \"def heuristics_v1(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n    return prize / np.sum(weight, axis=1)\\n\\n\\nRefer to the format of a trivial design above. Be very creative and give `heuristics_v2`. Output code only and enclose your code with Python code block: ```python ... ```.\\n\"}, \"type\": \"mutate\", \"population\": [{\"stdout_filepath\": \"problem_iter5_response8.txt_stdout.txt\", \"code_path\": \"problem_iter5_code8.py\", \"code\": \"import numpy as np\\nimport numpy as np\\n\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n    n, m = weight.shape\\n    # Normalize prize for comparison\\n    normalized_prize = prize / np.max(prize)\\n    \\n    # Calculate prize to weight ratio for each dimension\\n    ratio = prize / np.maximum(np.sum(weight, axis=1), 1e-5)\\n    \\n    # Calculate weight impact penalty based on utilization\\n    total_weight = np.sum(weight, axis=0)\\n    diversity_factor = np.exp(-total_weight / n)  # Penalize for over-utilization: higher utilization -> lower factor\\n    \\n    # Calculate a rarity factor based on how uniquely an item's weight distributes across dimensions\\n    rarity_factor = 1 / (np.std(weight, axis=1) + 1e-5)\\n    \\n    # Weighted combination of factors to form the final heuristic\\n    heuristics = normalized_prize * ratio * np.dot(weight, diversity_factor) * rarity_factor\\n    \\n    return heuristics\", \"response_id\": 8, \"obj\": -22.459772371769624, \"exec_success\": true}, {\"stdout_filepath\": \"problem_iter10_response0.txt_stdout.txt\", \"code_path\": \"problem_iter10_code0.py\", \"code\": \"import numpy as np\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n\\n    n, m = weight.shape\\n\\n    # Prize-to-weight ratio, adjusted for multi-dimensionality and focusing on the most constrained dimension\\n    ratios = prize / np.max(weight, axis=1)\\n\\n    # Dynamic penalty based on current knapsack utilization\\n    weights_sum = np.sum(weight, axis=0)\\n    penalty_factor = 1 / (1 + np.max(weights_sum))  # Higher utilization -> higher penalty\\n    weighted_penalty = np.dot(weight, penalty_factor)\\n\\n    # Item correlation and rarity (frequency of weight values across items)\\n    weight_counts = {}\\n    for w in weight.flatten():\\n        weight_counts[w] = weight_counts.get(w, 0) + 1\\n    rarity = np.array([1 / (weight_counts[w] + 1e-5) for w in weight.flatten()])\\n    rarity = rarity.reshape((n, m))\\n    rarity_factor = np.mean(rarity, axis=1)  # Mean rarity across dimensions\\n\\n    # Adaptive balancing of reward (ratios) and penalty based on knapsack utilization\\n    current_utilization = np.mean(weights_sum)\\n    balance_factor = current_utilization / (1 + current_utilization)  # Adjust influence based on how full knapsacks are\\n\\n    # Combine heuristics (avoiding over-normalization)\\n    heuristics = ratios * rarity_factor - balance_factor * weighted_penalty\\n\\n    # Normalize heuristics to ensure sum is 1, which is expected for sampling probabilities\\n    return heuristics / np.sum(heuristics) if np.sum(heuristics) > 0 else heuristics\", \"response_id\": 0, \"exec_success\": false, \"obj\": Infinity, \"traceback_msg\": \"Traceback (most recent call last):\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 58, in <module>\\n    obj = solve(prize, weight)\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 23, in solve\\n    heu = heuristics(prize.copy(), weight.copy()) + 1e-9\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/gpt.py\\\", line 27, in heuristics_v2\\nValueError: operands could not be broadcast together with shapes (100,) (100,5) \\n\"}, {\"stdout_filepath\": \"problem_iter10_response1.txt_stdout.txt\", \"code_path\": \"problem_iter10_code1.py\", \"code\": \"import numpy as np\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n    n, m = weight.shape\\n    \\n    # Normalize prize to be between 0 and 1\\n    normalized_prize = prize / (np.max(prize) + 1e-5)\\n\\n    # Calculate effective weight as the sum for each item\\n    effective_weight = np.sum(weight, axis=1)\\n\\n    # Calculate prize-to-effective-weight ratio, avoiding division by zero\\n    ratio = normalized_prize / (effective_weight + 1e-5)\\n    \\n    # Utilize both the sum and the maximum of weights for a balance between dimensions\\n    weight_sum = np.sum(weight, axis=0)\\n    weight_max = np.max(weight, axis=0)\\n    weight_penalty = (weight_sum / (weight_max + 1e-5)) + 1  # Penalize based on weight distribution\\n\\n    # Calculate a rarity factor based on uniqueness of weights\\n    rarity_factor = 1 / (np.std(weight, axis=1) + 1e-5)\\n\\n    # Heuristic: combine normalized prize, weight ratio, and rarity factor. \\n    heuristics = ratio * rarity_factor / weight_penalty\\n    \\n    return heuristics\", \"response_id\": 1, \"exec_success\": false, \"obj\": Infinity, \"traceback_msg\": \"Traceback (most recent call last):\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 58, in <module>\\n    obj = solve(prize, weight)\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 23, in solve\\n    heu = heuristics(prize.copy(), weight.copy()) + 1e-9\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/gpt.py\\\", line 23, in heuristics_v2\\nValueError: operands could not be broadcast together with shapes (100,) (5,) \\n\"}, {\"stdout_filepath\": \"problem_iter10_response2.txt_stdout.txt\", \"code_path\": \"problem_iter10_code2.py\", \"code\": \"import numpy as np\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n    n, m = weight.shape\\n    # Normalize prize using min-max scaling\\n    normalized_prize = (prize - np.min(prize)) / (np.max(prize) - np.min(prize))\\n    \\n    # Calculate prize to weight ratio for each dimension\\n    ratio = normalized_prize / np.maximum(np.sum(weight, axis=1), 1e-5)\\n    \\n    # Calculate weight impact penalty based on utilization\\n    total_weight = np.sum(weight, axis=0)\\n    diversity_factor = np.exp(-total_weight / n)  # Penalize for over-utilization: higher utilization -> lower factor\\n    \\n    # Calculate a rarity factor based on how uniquely an item's weight distributes across dimensions\\n    rarity_factor = 1 / (np.std(weight, axis=1) + 1e-5)\\n    \\n    # Weighted combination of factors to form the final heuristic\\n    heuristics = normalized_prize * ratio * np.dot(weight, diversity_factor) * rarity_factor\\n    \\n    return heuristics\", \"response_id\": 2, \"obj\": -22.43995378677538, \"exec_success\": true}, {\"stdout_filepath\": \"problem_iter10_response3.txt_stdout.txt\", \"code_path\": \"problem_iter10_code3.py\", \"code\": \"import numpy as np\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray, historical_data: np.ndarray) -> np.ndarray:\\n\\n    n, m = weight.shape\\n    # Normalize prize for comparison\\n    normalized_prize = prize / np.max(prize)\\n\\n    # Calculate prize to weight ratio for each dimension\\n    ratio = prize / np.maximum(np.sum(weight, axis=1), 1e-5)\\n\\n    # Calculate weight impact penalty based on utilization\\n    total_weight = np.sum(weight, axis=0)\\n    diversity_factor = np.exp(-total_weight / n)  # Penalize for over-utilization: higher utilization -> lower factor\\n\\n    # Calculate a rarity factor based on how uniquely an item's weight distributes across dimensions\\n    rarity_factor = 1 / (np.std(weight, axis=1) + 1e-5)\\n\\n    # Incorporate historical performance data for adjustments\\n    performance_factor = 1 + (historical_data - np.mean(historical_data)) / (np.std(historical_data) + 1e-5)\\n\\n    # Experiment with dynamic scaling factors\\n    scaling_factor = np.random.beta(2, 2)  # Beta distribution between 0 and 1\\n\\n    # Introduce diversity promoting mechanisms\\n    item_correlation = np.corrcoef(weight.T)  # Correlation matrix of items\\n    correlation_penalty = np.abs(item_correlation - np.eye(m))  # Absolute difference from identity matrix, i.e., penalize correlated items\\n    correlation_penalty = np.sum(correlation_penalty, axis=1)  # Sum along columns\\n    diversity_penalty = np.exp(-correlation_penalty / n)  # Penalize for correlation: higher correlation -> lower penalty\\n\\n    # Non-linear combination of factors\\n    heuristics = np.power(normalized_prize, 2) * ratio * np.power(np.dot(weight, diversity_factor), 2) * rarity_factor * performance_factor * diversity_penalty * scaling_factor\\n\\n    return heuristics\", \"response_id\": 3, \"exec_success\": false, \"obj\": Infinity, \"traceback_msg\": \"Traceback (most recent call last):\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 58, in <module>\\n    obj = solve(prize, weight)\\n  File \\\"/mnt/d/backup/data/AI Hota/TinhToanTienHoa/reevo/problems/mkp_aco/eval.py\\\", line 23, in solve\\n    heu = heuristics(prize.copy(), weight.copy()) + 1e-9\\nTypeError: heuristics_v2() missing 1 required positional argument: 'historical_data'\\n\"}, {\"stdout_filepath\": \"problem_iter10_response4.txt_stdout.txt\", \"code_path\": \"problem_iter10_code4.py\", \"code\": \"import numpy as np\\ndef heuristics_v2(prize: np.ndarray, weight: np.ndarray) -> np.ndarray:\\n\\n    n, m = weight.shape\\n    # Normalize prize for comparison\\n    normalized_prize = prize / np.max(prize)\\n    \\n    # Calculate prize to weight ratio for each dimension\\n    ratio = prize / np.maximum(np.sum(weight, axis=1), 1e-5)\\n    \\n    # Calculate weight impact penalty based on utilization\\n    total_weight = np.sum(weight, axis=0)\\n    diversity_factor = np.exp(-total_weight / n)  # Penalize for over-utilization: higher utilization -> lower factor\\n    \\n    # Calculate a rarity factor based on how uniquely an item's weight distributes across dimensions\\n    rarity_factor = 1 / (np.std(weight, axis=1) + 1e-5)\\n    \\n    # Weighted combination of factors to form the final heuristic\\n    heuristics = normalized_prize * ratio * np.dot(weight, diversity_factor) * rarity_factor\\n    \\n    return heuristics\", \"response_id\": 4, \"obj\": -22.417554576085752, \"exec_success\": true}], \"memory_variables\": {\"long_term_reflection_str\": \"Prioritize critical constraints and relative metrics (prize/weight within dimensions, knapsack utilization).  Balance exploration (rarity, correlations, diverse penalties) with exploitation (adaptive reward/penalty scaling, historical data). Consider non-linear factor combinations.\\n\", \"external_knowledge\": \"\"}}","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.1.0"},"id":"TextInput-yoLpb"},"selected":false,"width":384,"height":289,"dragging":false},{"id":"APIRequest-DQxQ6","type":"genericNode","position":{"x":4802.042940069741,"y":3025.242762261157},"data":{"type":"APIRequest","node":{"template":{"_type":"Component","query_params":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"query_params","value":"","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput"},"body":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"body","value":{},"display_name":"Body","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. \"\n            \"This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). \"\n            \"This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError:\n                    logger.exception(\"Error decoding JSON data\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            msg = f\"Error parsing curl: {exc}\"\n            logger.exception(msg)\n            raise ValueError(msg) from exc\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: dict | None = None,\n        body: dict | None = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            msg = f\"Unsupported method: {method}\"\n            raise ValueError(msg)\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                msg = f\"Error decoding JSON data: {e}\"\n                logger.exception(msg)\n                body = None\n                raise ValueError(msg) from e\n\n        data = body or None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:  # noqa: BLE001\n                logger.opt(exception=True).debug(\"Error decoding JSON response\")\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:  # noqa: BLE001\n            logger.opt(exception=True).debug(f\"Error making request to {url}\")\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> list[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[\n                    self.make_request(client, method, u, headers, rec, timeout)\n                    for u, rec in zip(urls, bodies, strict=True)\n                ]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"curl":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"curl","value":"","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput"},"headers":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"headers","value":{},"display_name":"Headers","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The headers to send with the request as a dictionary. This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"method":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"method","value":"POST","display_name":"Method","advanced":false,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":5,"display_name":"Timeout","advanced":false,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput"},"urls":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"urls","value":["https://eoldm90e3t999g2.m.pipedream.net"],"display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"This component allows you to make HTTP requests to one or more URLs. You can provide headers and body as either dictionaries or Data objects. Additionally, you can append query parameters to the URLs.\n\n**Note:** Check advanced options for more settings.","icon":"Globe","base_classes":["Data"],"display_name":"API Request","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"make_requests","value":"__UNDEFINED__","cache":true}],"field_order":["urls","curl","method","headers","body","query_params","timeout"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"APIRequest-DQxQ6"},"selected":false,"width":384,"height":883},{"id":"MessagetoData-vwpxD","type":"genericNode","position":{"x":4261.437202247713,"y":3205.675543480611},"data":{"type":"MessagetoData","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from loguru import logger\n\nfrom langflow.custom import Component\nfrom langflow.io import MessageInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass MessageToDataComponent(Component):\n    display_name = \"Message to Data\"\n    description = \"Convert a Message object to a Data object\"\n    icon = \"message-square-share\"\n    beta = True\n    name = \"MessagetoData\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_message\",\n            display_name=\"Input\",\n        ),\n        MessageInput(\n            name=\"output_message\",\n            display_name=\"Output\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"convert_message_to_data\"),\n    ]\n\n    def convert_message_to_data(self) -> Data:\n        try:\n            # Convert Message to Data\n            data = Data(\n                input=self.input_message.text,\n                output_message=self.output_message.text,\n            )\n\n            self.status = \"Successfully converted Message to Data\"\n            return data\n        except Exception as e:  # noqa: BLE001\n            error_message = f\"Error converting Message to Data: {e}\"\n            logger.opt(exception=True).debug(error_message)\n            self.status = error_message\n            return Data(data={\"error\": error_message})\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_message","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"output_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"output_message","value":"","display_name":"Output","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"}},"description":"Convert a Message object to a Data object","icon":"message-square-share","base_classes":["Data"],"display_name":"Message to Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"convert_message_to_data","value":"__UNDEFINED__","cache":true}],"field_order":["input_message","output_message"],"beta":true,"edited":true,"metadata":{},"lf_version":"1.0.19"},"id":"MessagetoData-vwpxD"},"selected":false,"width":384,"height":375},{"id":"CustomComponent-uL04l","type":"genericNode","position":{"x":3732.272355548255,"y":2115.7788877299263},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","crossover_fun":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"crossover_fun","value":"","display_name":"Crossover","advanced":false,"input_types":["Callable"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"init_population_fun":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"init_population_fun","value":"","display_name":"Init population","advanced":false,"input_types":["Callable"],"dynamic":false,"info":"Init population function","title_case":false,"type":"other","_input_type":"HandleInput"},"input_value":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"JSON object","title_case":false,"type":"other","_input_type":"HandleInput"},"llm_model":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"llm_model","value":"","display_name":"LLM Model","advanced":false,"input_types":["Callable"],"dynamic":false,"info":"Language Model to use for the component.","title_case":false,"type":"other","_input_type":"HandleInput"},"mutate_fun":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mutate_fun","value":"","display_name":"Mutate","advanced":false,"input_types":["Callable"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\r\nimport json\r\nfrom langflow.custom import Component\r\nfrom langflow.io import MessageTextInput, Output, HandleInput\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\n\r\nclass CustomComponent(Component):\r\n    display_name = \"Custom Component\"\r\n    description = \"Use as a template to create your own component.\"\r\n    documentation: str = \"http://docs.langflow.org/components/custom\"\r\n    icon = \"custom_components\"\r\n    name = \"CustomComponent\"\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_value\",\r\n            display_name=\"Input\",\r\n            info=\"JSON object\",\r\n            required=True,\r\n            input_types=[\"Message\"],\r\n        ),\r\n        HandleInput(\r\n            name=\"llm_model\",\r\n            display_name=\"LLM Model\",\r\n            info=\"Language Model to use for the component.\",\r\n            input_types=[\"Callable\"],\r\n        ),\r\n        HandleInput(\r\n            name=\"init_population_fun\",\r\n            display_name=\"Init population\",\r\n            info=\"Init population function\",\r\n            input_types=[\"Callable\"],\r\n        ),\r\n        HandleInput(\r\n            name=\"crossover_fun\",\r\n            display_name=\"Crossover\",\r\n            input_types=[\"Callable\"],\r\n        ),\r\n        HandleInput(\r\n            name=\"mutate_fun\",\r\n            display_name=\"Mutate\",\r\n            input_types=[\"Callable\"],\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\r\n    ]\r\n\r\n    def build_output(self) -> Message:\r\n        json_data = json.loads(self.input_value.text)\r\n        self.my_data = json_data\r\n\r\n        class MemoryVariables:\r\n            def __init__(\r\n                self,\r\n                long_term_reflection_str: str = \"\",\r\n                external_knowledge: str = \"\",\r\n            ):\r\n                self.long_term_reflection_str = long_term_reflection_str\r\n                self.external_knowledge = external_knowledge\r\n\r\n        self.memory_variables = MemoryVariables(\r\n            **json_data[\"memory_variables\"])\r\n\r\n        res = []\r\n        if self.my_data[\"type\"] == \"init_population\":\r\n            res = self.init_population_fun(self)\r\n        elif self.my_data[\"type\"] == \"crossover\":\r\n            res = self.crossover_fun(self)\r\n        elif self.my_data[\"type\"] == \"mutate\":\r\n            res = self.mutate_fun(self)\r\n\r\n        return Message(text=json.dumps({\r\n            \"population\": res,\r\n            \"variables\": self.memory_variables.__dict__,\r\n        }))\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Message"],"display_name":"CoreProcess","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","llm_model","init_population_fun","crossover_fun","mutate_fun"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"CustomComponent-uL04l"},"selected":false,"width":384,"height":459,"dragging":false},{"id":"JSONCleaner-mWIUq","type":"genericNode","position":{"x":1721.0915199419096,"y":2365.8946388528466},"data":{"type":"JSONCleaner","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import json\r\nimport re\r\nimport unicodedata\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.inputs import BoolInput, MessageTextInput\r\nfrom langflow.schema.message import Message\r\nfrom langflow.template import Output\r\n\r\n\r\nclass JSONCleaner(Component):\r\n    display_name = \"JSON Cleaner\"\r\n    description = (\r\n        \"Cleans the messy and sometimes incorrect JSON strings produced by LLMs \"\r\n        \"so that they are fully compliant with the JSON spec.\"\r\n    )\r\n    icon = \"custom_components\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"json_str\", display_name=\"JSON String\", info=\"The JSON string to be cleaned.\", required=True\r\n        ),\r\n        BoolInput(\r\n            name=\"remove_control_chars\",\r\n            display_name=\"Remove Control Characters\",\r\n            info=\"Remove control characters from the JSON string.\",\r\n            required=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"normalize_unicode\",\r\n            display_name=\"Normalize Unicode\",\r\n            info=\"Normalize Unicode characters in the JSON string.\",\r\n            required=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"validate_json\",\r\n            display_name=\"Validate JSON\",\r\n            info=\"Validate the JSON string to ensure it is well-formed.\",\r\n            required=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Cleaned JSON String\", name=\"output\", method=\"clean_json\"),\r\n    ]\r\n\r\n    def clean_json(self) -> Message:\r\n        try:\r\n            from json_repair import repair_json\r\n        except ImportError as e:\r\n            msg = \"Could not import the json_repair package. Please install it with `pip install json_repair`.\"\r\n            raise ImportError(msg) from e\r\n\r\n        \"\"\"Clean the input JSON string based on provided options and return the cleaned JSON string.\"\"\"\r\n        json_str = self.json_str\r\n        remove_control_chars = self.remove_control_chars\r\n        normalize_unicode = self.normalize_unicode\r\n        validate_json = self.validate_json\r\n\r\n        try:\r\n            start = json_str.find(\"{\")\r\n            end = json_str.rfind(\"}\")\r\n            if start == -1 or end == -1:\r\n                msg = \"Invalid JSON string: Missing '{' or '}'\"\r\n                raise ValueError(msg)\r\n            json_str = json_str[start : end + 1]\r\n\r\n            if remove_control_chars:\r\n                json_str = self._remove_control_characters(json_str)\r\n            if normalize_unicode:\r\n                json_str = self._normalize_unicode(json_str)\r\n            if validate_json:\r\n                json_str = self._validate_json(json_str)\r\n\r\n            cleaned_json_str = repair_json(json_str)\r\n            result = str(cleaned_json_str)\r\n\r\n            self.status = result\r\n            return Message(text=result)\r\n        except Exception as e:\r\n            msg = f\"Error cleaning JSON string: {e}\"\r\n            raise ValueError(msg) from e\r\n\r\n    def _remove_control_characters(self, s: str) -> str:\r\n        \"\"\"Remove control characters from the string.\"\"\"\r\n        # return re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", s)\r\n        # Replance \\n to \\\\n\r\n        return s.replace(\"\\n\", \"\\\\n\")\r\n\r\n    def _normalize_unicode(self, s: str) -> str:\r\n        \"\"\"Normalize Unicode characters in the string.\"\"\"\r\n        return unicodedata.normalize(\"NFC\", s)\r\n\r\n    def _validate_json(self, s: str) -> str:\r\n        \"\"\"Validate the JSON string.\"\"\"\r\n        try:\r\n            json.loads(s)\r\n            return s\r\n        except json.JSONDecodeError as e:\r\n            msg = f\"Invalid JSON string: {e}\"\r\n            raise ValueError(msg) from e\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"json_str":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"name":"json_str","value":"","display_name":"JSON String","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The JSON string to be cleaned.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"normalize_unicode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"normalize_unicode","value":false,"display_name":"Normalize Unicode","advanced":false,"dynamic":false,"info":"Normalize Unicode characters in the JSON string.","title_case":false,"type":"bool","_input_type":"BoolInput"},"remove_control_chars":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"remove_control_chars","value":true,"display_name":"Remove Control Characters","advanced":false,"dynamic":false,"info":"Remove control characters from the JSON string.","title_case":false,"type":"bool","_input_type":"BoolInput","load_from_db":false},"validate_json":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"validate_json","value":true,"display_name":"Validate JSON","advanced":false,"dynamic":false,"info":"Validate the JSON string to ensure it is well-formed.","title_case":false,"type":"bool","_input_type":"BoolInput","load_from_db":false}},"description":"Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.","icon":"custom_components","base_classes":["Message"],"display_name":"JSON Cleaner","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Cleaned JSON String","method":"clean_json","value":"__UNDEFINED__","cache":true}],"field_order":["json_str","remove_control_chars","normalize_unicode","validate_json"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"JSONCleaner-mWIUq"},"selected":false,"width":384,"height":537},{"id":"CustomComponent-aLjUO","type":"genericNode","position":{"x":1996.0951948659113,"y":1455.1430884053777},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","llm_models":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"llm_models","value":"","display_name":"LLM Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"Language Model to use for the component.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.custom.utils import get_function\nfrom collections.abc import Callable\nimport random\n\n\nclass CustomComponent(Component):\n    display_name = \"Custom Component\"\n    description = \"Use as a template to create your own component.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"llm_models\",\n            display_name=\"LLM Model\",\n            info=\"Language Model to use for the component.\",\n            input_types=[\"LanguageModel\"],\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Callable:\n        def run(messages):\n            tried_time = 3\n            while tried_time > 0:\n                tried_time -= 1\n                index = random.randrange(0, len(self.llm_models))\n                try:\n                    return self.llm_models[index].invoke(messages)\n                except:\n                    continue\n        \n        self.status = run\n        return run\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Callable"],"display_name":"MixModel","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Callable"],"selected":"Callable","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["llm_models"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"CustomComponent-aLjUO"},"selected":false,"width":384,"height":267,"dragging":false,"positionAbsolute":{"x":1996.0951948659113,"y":1455.1430884053777}},{"id":"CustomComponent-R2Yt9","type":"genericNode","position":{"x":2992.1644456191775,"y":2466.3220865810063},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\r\nimport json\r\nfrom langflow.custom import Component\r\nfrom langflow.io import MessageTextInput, Output, HandleInput\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\nfrom langflow.custom.utils import get_function\r\nfrom collections.abc import Callable\r\n\r\n\r\nclass CustomComponent(Component):\r\n    display_name = \"Custom Component\"\r\n    description = \"Use as a template to create your own component.\"\r\n    documentation: str = \"http://docs.langflow.org/components/custom\"\r\n    icon = \"custom_components\"\r\n    name = \"CustomComponent\"\r\n\r\n    inputs = []\r\n\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\r\n    ]\r\n\r\n    def build_output(self) -> Callable:\r\n        def init_population(self) -> list[str]:\r\n            population = []\r\n            for _ in range(self.my_data[\"cfg\"][\"init_pop_size\"]):\r\n                system = self.my_data[\"info\"][\"system_generator_prompt\"]\r\n                user = self.my_data[\"info\"][\"user_generator_prompt\"] + \"\\n\" + \\\r\n                    self.my_data[\"info\"][\"seed_prompt\"] + \"\\n\" + \\\r\n                    self.memory_variables.long_term_reflection_str\r\n    \r\n                messages = [\r\n                    (\"system\", system),\r\n                    (\"human\", user),\r\n                ]\r\n                response = self.llm_model(messages).content\r\n                population.append(response)\r\n            return population\r\n            \r\n        self.status = init_population\r\n        return init_population\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Callable"],"display_name":"Init population","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Callable"],"selected":"Callable","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":[],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"CustomComponent-R2Yt9"},"selected":false,"width":384,"height":219,"positionAbsolute":{"x":2992.1644456191775,"y":2466.3220865810063},"dragging":false},{"id":"CustomComponent-pRNuJ","type":"genericNode","position":{"x":2988.3400219517252,"y":2710.253049572465},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\r\nimport json\r\nfrom langflow.custom import Component\r\nfrom langflow.io import MessageTextInput, Output, HandleInput\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\nfrom langflow.custom.utils import get_function\r\nfrom collections.abc import Callable\r\n\r\nclass CustomComponent(Component):\r\n    display_name = \"Custom Component\"\r\n    description = \"Use as a template to create your own component.\"\r\n    documentation: str = \"http://docs.langflow.org/components/custom\"\r\n    icon = \"custom_components\"\r\n    name = \"CustomComponent\"\r\n\r\n    inputs = []\r\n\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\r\n    ]\r\n\r\n    def build_output(self) -> Callable:\r\n        def crossover(self) -> list[dict]:\r\n            def filter_code(code_string):\r\n                \"\"\"Remove lines containing signature and import statements.\"\"\"\r\n                lines = code_string.split('\\n')\r\n                filtered_lines = []\r\n                for line in lines:\r\n                    if line.startswith('def'):\r\n                        continue\r\n                    elif line.startswith('import'):\r\n                        continue\r\n                    elif line.startswith('from'):\r\n                        continue\r\n                    elif line.startswith('return'):\r\n                        filtered_lines.append(line)\r\n                        break\r\n                    else:\r\n                        filtered_lines.append(line)\r\n                code_string = '\\n'.join(filtered_lines)\r\n                return code_string\r\n\r\n            def gen_short_term_reflection_prompt(ind1: dict, ind2: dict) -> tuple[list[dict], str, str]:\r\n                \"\"\"\r\n                Short-term reflection before crossovering two individuals.\r\n                \"\"\"\r\n                if ind1[\"obj\"] == ind2[\"obj\"]:\r\n                    print(ind1[\"code\"], ind2[\"code\"])\r\n                    raise ValueError(\r\n                        \"Two individuals to crossover have the same objective value!\")\r\n                # Determine which individual is better or worse\r\n                if ind1[\"obj\"] < ind2[\"obj\"]:\r\n                    better_ind, worse_ind = ind1, ind2\r\n                elif ind1[\"obj\"] > ind2[\"obj\"]:\r\n                    better_ind, worse_ind = ind2, ind1\r\n        \r\n                worse_code = filter_code(worse_ind[\"code\"])\r\n                better_code = filter_code(better_ind[\"code\"])\r\n        \r\n                system = self.my_data[\"info\"][\"system_reflector_prompt\"]\r\n                user = self.my_data[\"info\"][\"user_reflector_st_prompt\"].format(\r\n                    func_name=self.my_data[\"info\"][\"func_name\"],\r\n                    func_desc=self.my_data[\"info\"][\"func_desc\"],\r\n                    problem_desc=self.my_data[\"info\"][\"problem_desc\"],\r\n                    worse_code=worse_code,\r\n                    better_code=better_code\r\n                )\r\n                message = [\r\n                    (\"system\", system),\r\n                    (\"human\", user),\r\n                ]\r\n        \r\n                return message, worse_code, better_code\r\n        \r\n            def short_term_reflection(population: list[dict]) -> tuple[list[list[dict]], list[str], list[str]]:\r\n                \"\"\"\r\n                Short-term reflection before crossovering two individuals.\r\n                \"\"\"\r\n                messages_lst = []\r\n                worse_code_lst = []\r\n                better_code_lst = []\r\n                for i in range(0, self.my_data[\"cfg\"][\"pop_size\"], 2):\r\n                    # Select two individuals\r\n                    parent_1 = population[i%len(population)]\r\n                    parent_2 = population[(i+1)%len(population)]\r\n                    \r\n                    if parent_1[\"obj\"] == parent_2[\"obj\"]:\r\n                        continue\r\n        \r\n                    # Short-term reflection\r\n                    messages, worse_code, better_code = gen_short_term_reflection_prompt(parent_1, parent_2)\r\n                    messages_lst.append(messages)\r\n                    worse_code_lst.append(worse_code)\r\n                    better_code_lst.append(better_code)\r\n        \r\n                # Asynchronously generate responses\r\n                response_lst = [\r\n                    self.llm_model(messages).content\r\n                    for messages in messages_lst\r\n                ]\r\n                return response_lst, worse_code_lst, better_code_lst\r\n        \r\n            def long_term_reflection(short_term_reflections: list[str]) -> None:\r\n                \"\"\"\r\n                Long-term reflection before mutation.\r\n                \"\"\"\r\n                system = self.my_data[\"info\"][\"system_reflector_prompt\"]\r\n                user = self.my_data[\"info\"][\"user_reflector_lt_prompt\"].format(\r\n                    problem_desc=self.my_data[\"info\"][\"problem_desc\"],\r\n                    prior_reflection=self.memory_variables.long_term_reflection_str,\r\n                    new_reflection=\"\\n\".join(short_term_reflections),\r\n                )\r\n                messages = [\r\n                    (\"system\", system),\r\n                    (\"human\", user),\r\n                ]\r\n        \r\n                self.memory_variables.long_term_reflection_str = self.llm_model(messages).content\r\n            \r\n            reflection_content_lst, worse_code_lst, better_code_lst = short_term_reflection(self.my_data[\"population\"])\r\n    \r\n            messages_lst = []\r\n            for reflection, worse_code, better_code in zip(reflection_content_lst, worse_code_lst, better_code_lst):\r\n                # Crossover\r\n                system = self.my_data[\"info\"][\"system_generator_prompt\"]\r\n                func_signature0 = self.my_data[\"info\"][\"func_signature\"].format(\r\n                    version=0)\r\n                func_signature1 = self.my_data[\"info\"][\"func_signature\"].format(\r\n                    version=1)\r\n                user = self.my_data[\"info\"][\"crossover_prompt\"].format(\r\n                    user_generator=self.my_data[\"info\"][\"user_generator_prompt\"],\r\n                    func_signature0=func_signature0,\r\n                    func_signature1=func_signature1,\r\n                    worse_code=worse_code,\r\n                    better_code=better_code,\r\n                    reflection=reflection,\r\n                    func_name=self.my_data[\"info\"][\"func_name\"],\r\n                )\r\n                messages = [\r\n                    (\"system\", system),\r\n                    (\"human\", user),\r\n                ]\r\n                messages_lst.append(messages)\r\n    \r\n            # Asynchronously generate responses\r\n            response_lst = [\r\n                self.llm_model(messages).content\r\n                for messages in messages_lst\r\n            ]\r\n    \r\n            # assert len(response_lst) == self.my_data[\"cfg\"][\"pop_size\"]\r\n    \r\n            # Long-term reflection\r\n            long_term_reflection(reflection_content_lst)\r\n    \r\n            return response_lst\r\n            \r\n        self.status = crossover\r\n        return crossover\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Callable"],"display_name":"Crossover","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Callable"],"selected":"Callable","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":[],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"CustomComponent-pRNuJ"},"selected":false,"width":384,"height":219,"dragging":false},{"id":"Pass-fxo0S","type":"genericNode","position":{"x":2984.470916522312,"y":1818.769425422618},"data":{"type":"Pass","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass PassMessageComponent(Component):\n    display_name = \"Pass\"\n    description = \"Forwards the input message, unchanged.\"\n    name = \"Pass\"\n    icon = \"arrow-right\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to be passed forward.\",\n        ),\n        MessageInput(\n            name=\"ignored_message\",\n            display_name=\"Ignored Message\",\n            info=\"A second message to be ignored. Used as a workaround for continuity.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        self.status = self.input_message\n        return self.input_message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"ignored_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"ignored_message","value":"","display_name":"Ignored Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"A second message to be ignored. Used as a workaround for continuity.","title_case":false,"type":"str","_input_type":"MessageInput"},"input_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_message","value":"","display_name":"Input Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The message to be passed forward.","title_case":false,"type":"str","_input_type":"MessageInput"}},"description":"Forwards the input message, unchanged.","icon":"arrow-right","base_classes":["Message"],"display_name":"Pass","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output_message","display_name":"Output Message","method":"pass_message","value":"__UNDEFINED__","cache":true}],"field_order":["input_message","ignored_message"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.1.0"},"id":"Pass-fxo0S"},"selected":false,"width":384,"height":289},{"id":"CustomComponent-FVL4D","type":"genericNode","position":{"x":2989.8089666661103,"y":2966.717657968943},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\r\nimport json\r\nfrom langflow.custom import Component\r\nfrom langflow.io import MessageTextInput, Output, HandleInput\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\nfrom langflow.custom.utils import get_function\r\nfrom collections.abc import Callable\r\n\r\n\r\nclass CustomComponent(Component):\r\n    display_name = \"Custom Component\"\r\n    description = \"Use as a template to create your own component.\"\r\n    documentation: str = \"http://docs.langflow.org/components/custom\"\r\n    icon = \"custom_components\"\r\n    name = \"CustomComponent\"\r\n\r\n    inputs = []\r\n\r\n    outputs = [\r\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\r\n    ]\r\n\r\n    def build_output(self) -> Callable:\r\n        def mutate(self) -> list[dict]:\r\n            \"\"\"Elitist-based mutation. We only mutate the best individual to generate n_pop new individuals.\"\"\"\r\n            def filter_code(code_string):\r\n                \"\"\"Remove lines containing signature and import statements.\"\"\"\r\n                lines = code_string.split('\\n')\r\n                filtered_lines = []\r\n                for line in lines:\r\n                    if line.startswith('def'):\r\n                        continue\r\n                    elif line.startswith('import'):\r\n                        continue\r\n                    elif line.startswith('from'):\r\n                        continue\r\n                    elif line.startswith('return'):\r\n                        filtered_lines.append(line)\r\n                        break\r\n                    else:\r\n                        filtered_lines.append(line)\r\n                code_string = '\\n'.join(filtered_lines)\r\n                return code_string\r\n            \r\n            system = self.my_data[\"info\"][\"system_generator_prompt\"]\r\n            func_signature1 = self.my_data[\"info\"][\"func_signature\"].format(\r\n                version=1)\r\n    \r\n            # Fine elitist code from population\r\n            best_obj = float(\"inf\")\r\n            elitist = None\r\n            for ind in self.my_data[\"population\"]:\r\n                if ind[\"obj\"] < best_obj:\r\n                    best_obj = ind[\"obj\"]\r\n                    elitist = ind\r\n    \r\n            user = self.my_data[\"info\"][\"mutataion_prompt\"].format(\r\n                user_generator=self.my_data[\"info\"][\"user_generator_prompt\"],\r\n                reflection=self.memory_variables.long_term_reflection_str +\r\n                self.memory_variables.external_knowledge,\r\n                func_signature1=func_signature1,\r\n                elitist_code=filter_code(elitist[\"code\"]),\r\n                func_name=self.my_data[\"info\"][\"func_name\"],\r\n            )\r\n            messages = [\r\n                (\"system\", system),\r\n                (\"human\", user),\r\n            ]\r\n    \r\n            population = [\r\n                self.llm_model(messages).content\r\n                for _ in range(self.my_data[\"cfg\"][\"pop_size\"])\r\n            ]\r\n            return population\r\n            \r\n        self.status = mutate\r\n        return mutate\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Callable"],"display_name":"Mutate","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Callable"],"selected":"Callable","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":[],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"CustomComponent-FVL4D"},"selected":false,"width":384,"height":219},{"id":"Pass-U8mWf","type":"genericNode","position":{"x":2991.225606701384,"y":2154.208068148159},"data":{"type":"Pass","node":{"template":{"_type":"Component","input_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"input_message","value":"","display_name":"Input Message","advanced":false,"input_types":["Callable"],"dynamic":false,"info":"The message to be passed forward.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass PassMessageComponent(Component):\n    display_name = \"Pass\"\n    description = \"Forwards the input message, unchanged.\"\n    name = \"Pass\"\n    icon = \"arrow-right\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to be passed forward.\",\n            input_types=[\"Callable\"],\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Callable:\n        self.status = self.input_message\n        return self.input_message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Forwards the input message, unchanged.","icon":"arrow-right","base_classes":["Callable"],"display_name":"Pass","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Callable"],"selected":"Callable","name":"output_message","display_name":"Output Message","method":"pass_message","value":"__UNDEFINED__","cache":true}],"field_order":["input_message"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.1.0"},"id":"Pass-U8mWf"},"selected":false,"width":384,"height":251},{"id":"Pass-LkIXL","type":"genericNode","position":{"x":2326.8566536140906,"y":3184.4600920596026},"data":{"type":"Pass","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass PassMessageComponent(Component):\n    display_name = \"Pass\"\n    description = \"Forwards the input message, unchanged.\"\n    name = \"Pass\"\n    icon = \"arrow-right\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to be passed forward.\",\n        ),\n        MessageInput(\n            name=\"ignored_message\",\n            display_name=\"Ignored Message\",\n            info=\"A second message to be ignored. Used as a workaround for continuity.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        self.status = self.input_message\n        return self.input_message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"ignored_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"ignored_message","value":"","display_name":"Ignored Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"A second message to be ignored. Used as a workaround for continuity.","title_case":false,"type":"str","_input_type":"MessageInput"},"input_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_message","value":"","display_name":"Input Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The message to be passed forward.","title_case":false,"type":"str","_input_type":"MessageInput"}},"description":"Forwards the input message, unchanged.","icon":"arrow-right","base_classes":["Message"],"display_name":"Pass","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output_message","display_name":"Output Message","method":"pass_message","value":"__UNDEFINED__","cache":true}],"field_order":["input_message","ignored_message"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.1.0"},"id":"Pass-LkIXL"},"selected":false,"width":384,"height":289},{"id":"GoogleGenerativeAIModel-ZHLR6","type":"genericNode","position":{"x":1427.0548970012628,"y":717.1536839826213},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\", \"gemini-1.5-flash-002\", \"gemini-1.5-pro-002\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"google_api_key","value":"","display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":8192,"display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"model":{"tool_mode":false,"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision","gemini-1.5-flash-002","gemini-1.5-pro-002"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"gemini-1.5-flash-002","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.9,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k","output_parser"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GoogleGenerativeAIModel-ZHLR6"},"selected":false,"width":384,"height":673,"positionAbsolute":{"x":1427.0548970012628,"y":717.1536839826213},"dragging":false},{"id":"GoogleGenerativeAIModel-srC3o","type":"genericNode","position":{"x":982.6515029966249,"y":727.7347171732079},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\", \"gemini-1.5-flash-002\", \"gemini-1.5-pro-002\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"google_api_key","value":"","display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":8192,"display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"model":{"tool_mode":false,"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision","gemini-1.5-flash-002","gemini-1.5-pro-002"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"gemini-1.5-flash-002","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.7,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k","output_parser"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GoogleGenerativeAIModel-srC3o"},"selected":false,"width":384,"height":673,"positionAbsolute":{"x":982.6515029966249,"y":727.7347171732079},"dragging":false},{"id":"GoogleGenerativeAIModel-aQdqn","type":"genericNode","position":{"x":522.3765592061088,"y":717.1536839826211},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\", \"gemini-1.5-flash-002\", \"gemini-1.5-pro-002\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"google_api_key","value":"","display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":8192,"display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"model":{"tool_mode":false,"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision","gemini-1.5-flash-002","gemini-1.5-pro-002"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"gemini-1.5-pro-002","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k","output_parser"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GoogleGenerativeAIModel-aQdqn"},"selected":false,"width":384,"height":673,"positionAbsolute":{"x":522.3765592061088,"y":717.1536839826211},"dragging":false},{"id":"GoogleGenerativeAIModel-Zto9f","type":"genericNode","position":{"x":50.197953076182216,"y":722.4442005779144},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\", \"gemini-1.5-flash-002\", \"gemini-1.5-pro-002\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"google_api_key","value":"","display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":8192,"display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"model":{"tool_mode":false,"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision","gemini-1.5-flash-002","gemini-1.5-pro-002"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"gemini-1.5-flash-002","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.5,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k","output_parser"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GoogleGenerativeAIModel-Zto9f"},"selected":false,"width":384,"height":673,"positionAbsolute":{"x":50.197953076182216,"y":722.4442005779144},"dragging":false},{"id":"OpenAIModel-Db0zz","type":"genericNode","position":{"x":1404.209564077095,"y":1533.5282166834033},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.7,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-Db0zz"},"selected":false,"width":384,"height":587,"positionAbsolute":{"x":1404.209564077095,"y":1533.5282166834033},"dragging":false},{"id":"OpenAIModel-EmndW","type":"genericNode","position":{"x":993.998043645486,"y":1525.9317070457812},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-EmndW"},"selected":false,"width":384,"height":587,"positionAbsolute":{"x":993.998043645486,"y":1525.9317070457812},"dragging":false},{"id":"OpenAIModel-PVtsI","type":"genericNode","position":{"x":554.9197865909111,"y":1510.7386877705364},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.3,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-PVtsI"},"selected":false,"width":384,"height":587,"positionAbsolute":{"x":554.9197865909111,"y":1510.7386877705364},"dragging":false},{"id":"OpenAIModel-00BTA","type":"genericNode","position":{"x":67.22386785555341,"y":1497.0649704228153},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-00BTA"},"selected":false,"width":384,"height":587,"positionAbsolute":{"x":67.22386785555341,"y":1497.0649704228153},"dragging":false},{"id":"GroqModel-weQ37","type":"genericNode","position":{"x":959.8964090193099,"y":48.86734184614534},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name=\"groq_api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\"),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return []\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"groq_api_key\", \"groq_api_base\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key).get_secret_value(),\n            streaming=stream,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["llama-3.2-90b-text-preview","llama-3.2-3b-preview","llama3-70b-8192","mixtral-8x7b-32768","llama-3.2-11b-text-preview","llama-3.1-8b-instant","gemma2-9b-it","llama-3.1-70b-versatile","distil-whisper-large-v3-en","whisper-large-v3-turbo","llama-guard-3-8b","gemma-7b-it","llama3-groq-8b-8192-tool-use-preview","llava-v1.5-7b-4096-preview","whisper-large-v3","llama3-8b-8192","llama-3.2-11b-vision-preview","llama-3.2-1b-preview","llama3-groq-70b-8192-tool-use-preview","llama-3.2-90b-vision-preview"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"mixtral-8x7b-32768","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.9,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GroqModel-weQ37"},"selected":false,"width":384,"height":603,"positionAbsolute":{"x":959.8964090193099,"y":48.86734184614534},"dragging":false},{"id":"GroqModel-ot79S","type":"genericNode","position":{"x":481.7062317991231,"y":34.90558484701572},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name=\"groq_api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\"),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return []\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"groq_api_key\", \"groq_api_base\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key).get_secret_value(),\n            streaming=stream,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["llama-3.2-90b-text-preview","llama-3.2-3b-preview","llama3-70b-8192","mixtral-8x7b-32768","llama-3.2-11b-text-preview","llama-3.1-8b-instant","gemma2-9b-it","llama-3.1-70b-versatile","distil-whisper-large-v3-en","whisper-large-v3-turbo","llama-guard-3-8b","gemma-7b-it","llama3-groq-8b-8192-tool-use-preview","llava-v1.5-7b-4096-preview","whisper-large-v3","llama3-8b-8192","llama-3.2-11b-vision-preview","llama-3.2-1b-preview","llama3-groq-70b-8192-tool-use-preview","llama-3.2-90b-vision-preview"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama-3.2-90b-text-preview","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.9,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GroqModel-ot79S"},"selected":false,"width":384,"height":603,"positionAbsolute":{"x":481.7062317991231,"y":34.90558484701572},"dragging":false},{"id":"GroqModel-hPk4v","type":"genericNode","position":{"x":35.27905175195576,"y":41.88646334658051},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name=\"groq_api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\"),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return []\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"groq_api_key\", \"groq_api_base\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key).get_secret_value(),\n            streaming=stream,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["llama-3.2-90b-text-preview","llama-3.2-3b-preview","llama3-70b-8192","mixtral-8x7b-32768","llama-3.2-11b-text-preview","llama-3.1-8b-instant","gemma2-9b-it","llama-3.1-70b-versatile","distil-whisper-large-v3-en","whisper-large-v3-turbo","llama-guard-3-8b","gemma-7b-it","llama3-groq-8b-8192-tool-use-preview","llava-v1.5-7b-4096-preview","whisper-large-v3","llama3-8b-8192","llama-3.2-11b-vision-preview","llama-3.2-1b-preview","llama3-groq-70b-8192-tool-use-preview","llama-3.2-90b-vision-preview"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama-3.2-11b-text-preview","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.9,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"GroqModel-hPk4v"},"selected":false,"width":384,"height":603,"positionAbsolute":{"x":35.27905175195576,"y":41.88646334658051},"dragging":false}],"edges":[{"source":"MessagetoData-vwpxD","target":"APIRequest-DQxQ6","sourceHandle":"{dataType:MessagetoData,id:MessagetoData-vwpxD,name:data,output_types:[Data]}","targetHandle":"{fieldName:body,id:APIRequest-DQxQ6,inputTypes:[Data],type:NestedDict}","id":"reactflow__edge-MessagetoData-vwpxD{dataType:MessagetoData,id:MessagetoData-vwpxD,name:data,output_types:[Data]}-APIRequest-DQxQ6{fieldName:body,id:APIRequest-DQxQ6,inputTypes:[Data],type:NestedDict}","data":{"targetHandle":{"fieldName":"body","id":"APIRequest-DQxQ6","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"MessagetoData","id":"MessagetoData-vwpxD","name":"data","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"CustomComponent-uL04l","target":"TextOutput-y81Tk","sourceHandle":"{dataType:CustomComponent,id:CustomComponent-uL04l,name:output,output_types:[Message]}","targetHandle":"{fieldName:input_value,id:TextOutput-y81Tk,inputTypes:[Message],type:str}","id":"reactflow__edge-CustomComponent-uL04l{dataType:CustomComponent,id:CustomComponent-uL04l,name:output,output_types:[Message]}-TextOutput-y81Tk{fieldName:input_value,id:TextOutput-y81Tk,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-y81Tk","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-uL04l","name":"output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-yoLpb","target":"JSONCleaner-mWIUq","sourceHandle":"{dataType:TextInput,id:TextInput-yoLpb,name:text,output_types:[Message]}","targetHandle":"{fieldName:json_str,id:JSONCleaner-mWIUq,inputTypes:[Message],type:str}","id":"reactflow__edge-TextInput-yoLpb{dataType:TextInput,id:TextInput-yoLpb,name:text,output_types:[Message]}-JSONCleaner-mWIUq{fieldName:json_str,id:JSONCleaner-mWIUq,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"json_str","id":"JSONCleaner-mWIUq","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-yoLpb","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"CustomComponent-pRNuJ","target":"CustomComponent-uL04l","sourceHandle":"{dataType:CustomComponent,id:CustomComponent-pRNuJ,name:output,output_types:[Callable]}","targetHandle":"{fieldName:crossover_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","id":"reactflow__edge-CustomComponent-pRNuJ{dataType:CustomComponent,id:CustomComponent-pRNuJ,name:output,output_types:[Callable]}-CustomComponent-uL04l{fieldName:crossover_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","data":{"targetHandle":{"fieldName":"crossover_fun","id":"CustomComponent-uL04l","inputTypes":["Callable"],"type":"other"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-pRNuJ","name":"output","output_types":["Callable"]}},"selected":false,"className":"","animated":false},{"source":"CustomComponent-FVL4D","target":"CustomComponent-uL04l","sourceHandle":"{dataType:CustomComponent,id:CustomComponent-FVL4D,name:output,output_types:[Callable]}","targetHandle":"{fieldName:mutate_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","id":"reactflow__edge-CustomComponent-FVL4D{dataType:CustomComponent,id:CustomComponent-FVL4D,name:output,output_types:[Callable]}-CustomComponent-uL04l{fieldName:mutate_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","data":{"targetHandle":{"fieldName":"mutate_fun","id":"CustomComponent-uL04l","inputTypes":["Callable"],"type":"other"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-FVL4D","name":"output","output_types":["Callable"]}},"selected":false,"className":"","animated":false},{"source":"Pass-fxo0S","target":"CustomComponent-uL04l","sourceHandle":"{dataType:Pass,id:Pass-fxo0S,name:output_message,output_types:[Message]}","targetHandle":"{fieldName:input_value,id:CustomComponent-uL04l,inputTypes:[Message],type:other}","id":"reactflow__edge-Pass-fxo0S{dataType:Pass,id:Pass-fxo0S,name:output_message,output_types:[Message]}-CustomComponent-uL04l{fieldName:input_value,id:CustomComponent-uL04l,inputTypes:[Message],type:other}","data":{"targetHandle":{"fieldName":"input_value","id":"CustomComponent-uL04l","inputTypes":["Message"],"type":"other"},"sourceHandle":{"dataType":"Pass","id":"Pass-fxo0S","name":"output_message","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"JSONCleaner-mWIUq","target":"Pass-fxo0S","sourceHandle":"{dataType:JSONCleaner,id:JSONCleaner-mWIUq,name:output,output_types:[Message]}","targetHandle":"{fieldName:input_message,id:Pass-fxo0S,inputTypes:[Message],type:str}","id":"reactflow__edge-JSONCleaner-mWIUq{dataType:JSONCleaner,id:JSONCleaner-mWIUq,name:output,output_types:[Message]}-Pass-fxo0S{fieldName:input_message,id:Pass-fxo0S,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_message","id":"Pass-fxo0S","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"JSONCleaner","id":"JSONCleaner-mWIUq","name":"output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"Pass-U8mWf","target":"CustomComponent-uL04l","sourceHandle":"{dataType:Pass,id:Pass-U8mWf,name:output_message,output_types:[Callable]}","targetHandle":"{fieldName:llm_model,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","id":"reactflow__edge-Pass-U8mWf{dataType:Pass,id:Pass-U8mWf,name:output_message,output_types:[Callable]}-CustomComponent-uL04l{fieldName:llm_model,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","data":{"targetHandle":{"fieldName":"llm_model","id":"CustomComponent-uL04l","inputTypes":["Callable"],"type":"other"},"sourceHandle":{"dataType":"Pass","id":"Pass-U8mWf","name":"output_message","output_types":["Callable"]}},"selected":false,"className":"","animated":false},{"source":"CustomComponent-aLjUO","target":"Pass-U8mWf","sourceHandle":"{dataType:CustomComponent,id:CustomComponent-aLjUO,name:output,output_types:[Callable]}","targetHandle":"{fieldName:input_message,id:Pass-U8mWf,inputTypes:[Callable],type:other}","id":"reactflow__edge-CustomComponent-aLjUO{dataType:CustomComponent,id:CustomComponent-aLjUO,name:output,output_types:[Callable]}-Pass-U8mWf{fieldName:input_message,id:Pass-U8mWf,inputTypes:[Callable],type:other}","data":{"targetHandle":{"fieldName":"input_message","id":"Pass-U8mWf","inputTypes":["Callable"],"type":"other"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-aLjUO","name":"output","output_types":["Callable"]}},"selected":false,"className":"","animated":false},{"source":"CustomComponent-R2Yt9","target":"CustomComponent-uL04l","sourceHandle":"{dataType:CustomComponent,id:CustomComponent-R2Yt9,name:output,output_types:[Callable]}","targetHandle":"{fieldName:init_population_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","id":"reactflow__edge-CustomComponent-R2Yt9{dataType:CustomComponent,id:CustomComponent-R2Yt9,name:output,output_types:[Callable]}-CustomComponent-uL04l{fieldName:init_population_fun,id:CustomComponent-uL04l,inputTypes:[Callable],type:other}","data":{"targetHandle":{"fieldName":"init_population_fun","id":"CustomComponent-uL04l","inputTypes":["Callable"],"type":"other"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-R2Yt9","name":"output","output_types":["Callable"]}},"selected":false,"className":"","animated":false},{"source":"JSONCleaner-mWIUq","target":"Pass-LkIXL","sourceHandle":"{dataType:JSONCleaner,id:JSONCleaner-mWIUq,name:output,output_types:[Message]}","targetHandle":"{fieldName:input_message,id:Pass-LkIXL,inputTypes:[Message],type:str}","id":"reactflow__edge-JSONCleaner-mWIUq{dataType:JSONCleaner,id:JSONCleaner-mWIUq,name:output,output_types:[Message]}-Pass-LkIXL{fieldName:input_message,id:Pass-LkIXL,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_message","id":"Pass-LkIXL","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"JSONCleaner","id":"JSONCleaner-mWIUq","name":"output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"GoogleGenerativeAIModel-ZHLR6","sourceHandle":"{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-ZHLR6,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-ZHLR6","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GoogleGenerativeAIModel-ZHLR6{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-ZHLR6,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GoogleGenerativeAIModel-srC3o","sourceHandle":"{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-srC3o,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-srC3o","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GoogleGenerativeAIModel-srC3o{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-srC3o,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GoogleGenerativeAIModel-aQdqn","sourceHandle":"{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-aQdqn,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-aQdqn","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GoogleGenerativeAIModel-aQdqn{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-aQdqn,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GoogleGenerativeAIModel-Zto9f","sourceHandle":"{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-Zto9f,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-Zto9f","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GoogleGenerativeAIModel-Zto9f{dataType:GoogleGenerativeAIModel,id:GoogleGenerativeAIModel-Zto9f,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"OpenAIModel-Db0zz","sourceHandle":"{dataType:OpenAIModel,id:OpenAIModel-Db0zz,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-Db0zz","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-OpenAIModel-Db0zz{dataType:OpenAIModel,id:OpenAIModel-Db0zz,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"OpenAIModel-EmndW","sourceHandle":"{dataType:OpenAIModel,id:OpenAIModel-EmndW,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-EmndW","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-OpenAIModel-EmndW{dataType:OpenAIModel,id:OpenAIModel-EmndW,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"OpenAIModel-PVtsI","sourceHandle":"{dataType:OpenAIModel,id:OpenAIModel-PVtsI,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-PVtsI","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-OpenAIModel-PVtsI{dataType:OpenAIModel,id:OpenAIModel-PVtsI,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"OpenAIModel-00BTA","sourceHandle":"{dataType:OpenAIModel,id:OpenAIModel-00BTA,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-00BTA","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-OpenAIModel-00BTA{dataType:OpenAIModel,id:OpenAIModel-00BTA,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GroqModel-hPk4v","sourceHandle":"{dataType:GroqModel,id:GroqModel-hPk4v,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-hPk4v","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-hPk4v{dataType:GroqModel,id:GroqModel-hPk4v,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GroqModel-ot79S","sourceHandle":"{dataType:GroqModel,id:GroqModel-ot79S,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-ot79S","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-ot79S{dataType:GroqModel,id:GroqModel-ot79S,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""},{"source":"GroqModel-weQ37","sourceHandle":"{dataType:GroqModel,id:GroqModel-weQ37,name:model_output,output_types:[LanguageModel]}","target":"CustomComponent-aLjUO","targetHandle":"{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm_models","id":"CustomComponent-aLjUO","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-weQ37","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-weQ37{dataType:GroqModel,id:GroqModel-weQ37,name:model_output,output_types:[LanguageModel]}-CustomComponent-aLjUO{fieldName:llm_models,id:CustomComponent-aLjUO,inputTypes:[LanguageModel],type:other}","animated":false,"className":""}],"viewport":{"x":257.9928304382022,"y":34.277179695615644,"zoom":0.21603998774456665}},"description":"Smart Chains, Smarter Conversations.","name":"TinhToanTienHoa","last_tested_version":"1.1.0","endpoint_name":null,"is_component":false}